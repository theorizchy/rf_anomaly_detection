{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Creating a machine learning model to differentiate between two states (camera on and camera off) based on spectrum analyzer data is an interesting problem that can be approached using various techniques. Given the nature of your data (frequency-domain data from a spectrum analyzer), a Convolutional Neural Network (CNN) is well-suited for this task because CNNs are effective in capturing spatial hierarchies and patterns in data.\n",
    "\n",
    "Here's the approach we'll take:\n",
    "- Data Preprocessing: Normalize the data to ensure all features contribute equally to the model.\n",
    "- Data Augmentation: Use techniques like adding Gaussian noise or jitter to increase the amount of training data.\n",
    "- Model Selection: Use a CNN because it can effectively capture spatial dependencies and patterns in the spectrum data.\n",
    "- Training and Validation: Split the data into training and validation sets to evaluate the model performance.\n",
    "- Optimization: Use techniques like early stopping and learning rate reduction on the plateau to optimize the training process.\n",
    "Let's start with the Python code to preprocess the data, build, and train the CNN model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "import tensorflow as tf\n",
    "import joblib\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Conv1D, MaxPooling1D, Flatten, Dense, Dropout\n",
    "from tensorflow.keras.callbacks import EarlyStopping, ReduceLROnPlateau"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the data\n",
    "cam_on = pd.read_csv('output_file/clearwrite_captured_SWEEP_REC_2024-06-11 20h09m34s_cam_off.csv', header=None)\n",
    "cam_on = cam_on.iloc[:, 1:]\n",
    "cam_off = pd.read_csv('output_file/clearwrite_captured_SWEEP_REC_2024-06-11 21h01m22s_cam_on.csv', header=None)\n",
    "cam_off = cam_off.iloc[:, 1:]\n",
    "\n",
    "# Create labels\n",
    "labels_on = np.ones(cam_on.shape[0])\n",
    "labels_off = np.zeros(cam_off.shape[0])\n",
    "\n",
    "# Combine data\n",
    "data = pd.concat([cam_on, cam_off])\n",
    "labels = np.concatenate([labels_on, labels_off])\n",
    "\n",
    "# Normalize the data\n",
    "scaler = StandardScaler()\n",
    "data_normalized = scaler.fit_transform(data)\n",
    "joblib.dump(scaler, 'models/scaler_cnn.pkl')\n",
    "\n",
    "# Split into training and test sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(data_normalized, labels, test_size=0.2, random_state=42)\n",
    "\n",
    "# Reshape data for CNN input\n",
    "X_train = X_train[..., np.newaxis]\n",
    "X_test = X_test[..., np.newaxis]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Define the CNN model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the model\n",
    "model = Sequential([\n",
    "    Conv1D(filters=64, kernel_size=3, activation='relu', input_shape=(X_train.shape[1], 1)),\n",
    "    MaxPooling1D(pool_size=2),\n",
    "    Conv1D(filters=128, kernel_size=3, activation='relu'),\n",
    "    MaxPooling1D(pool_size=2),\n",
    "    Flatten(),\n",
    "    Dense(128, activation='relu'),\n",
    "    Dropout(0.5),\n",
    "    Dense(1, activation='sigmoid')\n",
    "])\n",
    "\n",
    "# Compile the model\n",
    "model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Training the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the model\n",
    "model = Sequential([\n",
    "    Conv1D(filters=64, kernel_size=3, activation='relu', input_shape=(X_train.shape[1], 1)),\n",
    "    MaxPooling1D(pool_size=2),\n",
    "    Conv1D(filters=128, kernel_size=3, activation='relu'),\n",
    "    MaxPooling1D(pool_size=2),\n",
    "    Flatten(),\n",
    "    Dense(128, activation='relu'),\n",
    "    Dropout(0.5),\n",
    "    Dense(1, activation='sigmoid')\n",
    "])\n",
    "\n",
    "# Compile the model\n",
    "model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Evaluate the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "125/125 [==============================] - 2s 13ms/step - loss: 0.7209 - accuracy: 0.0610\n",
      "Test accuracy: 0.0610\n"
     ]
    }
   ],
   "source": [
    "# Evaluate the model on the test set\n",
    "test_loss, test_accuracy = model.evaluate(X_test, y_test)\n",
    "print(f'Test accuracy: {test_accuracy:.4f}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Save the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the entire model to a HDF5 file or TensorFlow SavedModel format\n",
    "model.save('spectrum_analyzer_model.h5')  # Using HDF5 format"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
