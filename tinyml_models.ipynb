{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Apply TinyML techniques to reduce model sizes\n",
    "\n",
    "Optimize the model using TensorFlow Lite and quantization. Convert the trained model to the TensorFlow Lite format and then apply quantization to reduce its size."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import classification_report\n",
    "\n",
    "import warnings\n",
    "# Suppress all warnings\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load back X_train, X_test, y_train, and y_test\n",
    "X_train = np.loadtxt('train_data/X_train_t10.txt')\n",
    "X_test = np.loadtxt('train_data/X_test_t10.txt')\n",
    "y_train = np.loadtxt('train_data/y_train_t10.txt')\n",
    "y_test = np.loadtxt('train_data/y_test_t10.txt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_scaled = X_train\n",
    "X_test_scaled = X_test"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create a Deep Neural-Networks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Model Selection and Training\n",
    "model = tf.keras.Sequential([\n",
    "    tf.keras.layers.Dense(128, activation='relu', input_shape=(X_train_scaled.shape[1],)),\n",
    "    tf.keras.layers.Dense(64, activation='relu'),\n",
    "    tf.keras.layers.Dense(1, activation='sigmoid')\n",
    "])\n",
    "\n",
    "model.compile(optimizer='rmsprop',\n",
    "              loss='binary_crossentropy',\n",
    "              metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Model Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "5/5 [==============================] - 1s 43ms/step - loss: 4752.9922 - accuracy: 0.5538 - val_loss: 1274.3019 - val_accuracy: 0.5455\n",
      "Epoch 2/10\n",
      "5/5 [==============================] - 0s 13ms/step - loss: 1008.7768 - accuracy: 0.4769 - val_loss: 1079.9604 - val_accuracy: 0.4545\n",
      "Epoch 3/10\n",
      "5/5 [==============================] - 0s 13ms/step - loss: 850.8342 - accuracy: 0.5231 - val_loss: 322.9302 - val_accuracy: 0.4545\n",
      "Epoch 4/10\n",
      "5/5 [==============================] - 0s 15ms/step - loss: 830.6862 - accuracy: 0.4154 - val_loss: 1181.3683 - val_accuracy: 0.5455\n",
      "Epoch 5/10\n",
      "5/5 [==============================] - 0s 13ms/step - loss: 760.0226 - accuracy: 0.4923 - val_loss: 437.0700 - val_accuracy: 0.4545\n",
      "Epoch 6/10\n",
      "5/5 [==============================] - 0s 13ms/step - loss: 338.5021 - accuracy: 0.5692 - val_loss: 807.1042 - val_accuracy: 0.5455\n",
      "Epoch 7/10\n",
      "5/5 [==============================] - 0s 14ms/step - loss: 574.1248 - accuracy: 0.4308 - val_loss: 261.2747 - val_accuracy: 0.5455\n",
      "Epoch 8/10\n",
      "5/5 [==============================] - 0s 13ms/step - loss: 301.0352 - accuracy: 0.5692 - val_loss: 296.2319 - val_accuracy: 0.5455\n",
      "Epoch 9/10\n",
      "5/5 [==============================] - 0s 13ms/step - loss: 357.6189 - accuracy: 0.4615 - val_loss: 708.2104 - val_accuracy: 0.4545\n",
      "Epoch 10/10\n",
      "5/5 [==============================] - 0s 13ms/step - loss: 402.9322 - accuracy: 0.4462 - val_loss: 1094.0490 - val_accuracy: 0.4545\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x1d13ca75b20>"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.fit(X_train_scaled, y_train, epochs=10, batch_size=32, validation_split=0.2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Model Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2/2 [==============================] - 0s 3ms/step\n"
     ]
    }
   ],
   "source": [
    "# Model Evaluation\n",
    "predictions = model.predict(X_test_scaled)\n",
    "predictions_binary = (predictions > 0.5).astype(int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       0.54      1.00      0.70        22\n",
      "         1.0       0.00      0.00      0.00        19\n",
      "\n",
      "    accuracy                           0.54        41\n",
      "   macro avg       0.27      0.50      0.35        41\n",
      "weighted avg       0.29      0.54      0.37        41\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(classification_report(y_test, predictions_binary))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: C:\\Users\\theor\\AppData\\Local\\Temp\\tmpdm63kev4\\assets\n"
     ]
    }
   ],
   "source": [
    "# Convert and save the TensorFlow model to TensorFlow Lite\n",
    "converter = tf.lite.TFLiteConverter.from_keras_model(model)\n",
    "tflite_model = converter.convert()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Save Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the TensorFlow Lite model to a file\n",
    "model_file_path = os.path.join('models', 'dnn_model.tflite')\n",
    "with open(model_file_path, \"wb\") as f:\n",
    "    f.write(tflite_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model is 39358480 bytes\n"
     ]
    }
   ],
   "source": [
    "model_size = os.path.getsize(model_file_path)\n",
    "print(\"Model is %d bytes\" % model_size)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Optimize the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: C:\\Users\\theor\\AppData\\Local\\Temp\\tmp66h_gopk\\assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: C:\\Users\\theor\\AppData\\Local\\Temp\\tmp66h_gopk\\assets\n"
     ]
    }
   ],
   "source": [
    "# Set optimizations (quantization)\n",
    "converter.optimizations = [tf.lite.Optimize.DEFAULT]\n",
    "# Convert the model\n",
    "tflite_model_optimize = converter.convert()\n",
    "\n",
    "model_file_path = os.path.join('models', 'model_optimize.tflite')\n",
    "with open(model_file_path, \"wb\") as f:\n",
    "    f.write(tflite_model_optimize)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Optimized Model is 9842064 bytes\n"
     ]
    }
   ],
   "source": [
    "optimized_model_size = os.path.getsize(model_file_path)\n",
    "print(\"Optimized Model is %d bytes\" % optimized_model_size)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Evaluate the optimized model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       1.00      1.00      1.00         1\n",
      "\n",
      "    accuracy                           1.00         1\n",
      "   macro avg       1.00      1.00      1.00         1\n",
      "weighted avg       1.00      1.00      1.00         1\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Load the optimized TensorFlow Lite model\n",
    "optimized_model_path = 'models/model_optimize.tflite'\n",
    "interpreter = tf.lite.Interpreter(model_path=optimized_model_path)\n",
    "interpreter.allocate_tensors()\n",
    "\n",
    "# Get input and output tensors\n",
    "input_details = interpreter.get_input_details()\n",
    "output_details = interpreter.get_output_details()\n",
    "\n",
    "# Prepare input data\n",
    "input_data = X_test_scaled[0:1].astype(np.float32)  # Take the first sample from X_test_scaled\n",
    "\n",
    "# Set input tensor\n",
    "interpreter.set_tensor(input_details[0]['index'], input_data)\n",
    "\n",
    "# Run inference\n",
    "interpreter.invoke()\n",
    "\n",
    "# Get the output tensor\n",
    "output_data = interpreter.get_tensor(output_details[0]['index'])\n",
    "\n",
    "# Convert output to binary predictions\n",
    "predictions_binary = (output_data > 0.5).astype(int)\n",
    "\n",
    "# Evaluate the predictions\n",
    "print(classification_report(y_test[0:1], predictions_binary))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Encode the Model to a Header File \n",
    "\n",
    "The next cell creates a constant byte array that contains the TFlite model. Import it as a tab with the sketch below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the input and output file paths\n",
    "input_file = 'tflite_model_optimize'\n",
    "output_file = './model_deployment/include/model.h'\n",
    "\n",
    "# Write the beginning of the model.h file\n",
    "with open(output_file, 'w') as f:\n",
    "    f.write(\"const unsigned char model[] = {\\n\")\n",
    "\n",
    "# Open file in append binary mode\n",
    "with open(output_file, 'ab') as f:\n",
    "    # Convert binary data to hexadecimal representation and write to the output file\n",
    "    hex_data = ', '.join(\"0x{:02x}\".format(byte) for byte in tflite_model_optimize)\n",
    "    f.write(bytes(hex_data, 'utf-8'))\n",
    "\n",
    "# Append the end of the model.h file\n",
    "with open(output_file, 'a') as f:\n",
    "    f.write(\"\\n};\\n\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
